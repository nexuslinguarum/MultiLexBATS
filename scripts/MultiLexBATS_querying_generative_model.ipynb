{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/MultiLexBATS/blob/main/scripts/MultiLexBATS_querying_generative_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihVR1G2Dmlm6"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analogical Reasoning Task**\n",
        "The following description relates to setting up the classical analogy task to be completed by a generative model. To be used in masked-type models, please consul the corresponding scripts on our GitHub.\n",
        "\n",
        "For this paper, we utilized the generative language model [BLOOM](https://huggingface.co/bigscience/bloom)."
      ],
      "metadata": {
        "id": "uvobVl6kubQM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gfu5wJ_iEIt"
      },
      "source": [
        "## Loading and Preprocessing MultiLexBATS\n",
        "\n",
        "Instead of running the analogy template on all available analogies, we randomly select 30 {a}, {b}, {c}, {d} pairs that we then run on all languages.\n",
        "\n",
        "To this end, the CSV per relation for all languages is loaded and a parallel dataset with no nan, DUPLICATE, or NO_TRANSLATION in any row is created. From this dataset, 30 random analogies are compiled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "KuWJCV05mvcX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def find_parallels_across_all_languages(df_parallel, df):\n",
        "  #Reshape data structure to make EN fit the format of the other languages and rename column Target to EN\n",
        "  df_parallel.loc[df_parallel['ID'].notna(), 'Target'] = df_parallel['Source']\n",
        "  df_parallel.rename(columns={'Target': 'EN'}, inplace = True)\n",
        "  df_parallel['ID'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "  df.loc[df['ID'].notna(), 'Target'] = df['Source']\n",
        "  df.rename(columns={'Target': 'EN'}, inplace = True)\n",
        "  df['ID'].fillna(method='ffill', inplace=True)\n",
        "\n",
        "  #Drop all rows with nan, DUPLICATE or NO_TRANSLATION in any language\n",
        "  df_parallel.dropna(subset=df_parallel.columns[4:], inplace=True)\n",
        "  for column in df_parallel.columns[4:]:\n",
        "    df_parallel = df_parallel[~df_parallel[column].str.contains('DUPLICATE|NO_TRANSLATION')]\n",
        "\n",
        "  #Only keep ideas that have parallel source words across all languages\n",
        "  all_valid_ids = df_parallel[df_parallel['Source'].notna()]['ID']\n",
        "  df_parallel = df_parallel[df_parallel['ID'].isin(all_valid_ids)]\n",
        "\n",
        "  return df_parallel, df\n",
        "\n",
        "def find_all_pairs(df_parallel, df, lang):\n",
        "  lang_dict_parallel, lang_dict = {}, {}\n",
        "  for i, row in df_parallel.iterrows():\n",
        "    if type(row['Source']) == str:\n",
        "      ID = row['ID']\n",
        "      en_source_word = row['EN'].replace(\"_\", \" \")\n",
        "      lang_source_word = row[lang].replace(\"_\", \" \")\n",
        "      lang_dict[ID] = {'source': lang_source_word, 'targets': []}\n",
        "      for x in list(df.loc[df['ID']==ID][lang])[1:]:\n",
        "        if type(x) == str and \"DUPLICATE\" not in x and \"NO_TRANSLATION\" not in x:\n",
        "          if \",\" in x.replace(\"_\", \" \"):\n",
        "            lang_dict[ID]['targets'].extend(x.replace(\"_\", \" \").split(\",\"))\n",
        "          else:\n",
        "            lang_dict[ID]['targets'].append(x.replace(\"_\", \" \"))\n",
        "    else:\n",
        "      if type(row['EN']) == str:\n",
        "        #Create one dictionary only containing pairs and one containing the source word and the set of all target words\n",
        "        lang_dict_parallel[ID+\"__\"+en_source_word+\"_\"+row['EN']] = [lang_source_word, row[lang].replace(\"_\", \" \")]\n",
        "\n",
        "  return lang_dict_parallel, lang_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Define Analogy Templates\n",
        "Use the following function to create a set of language-specific templates for the classical anlogy task."
      ],
      "metadata": {
        "id": "M02HD2R2tMKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_template_with_quotes(lang, a, b, c):\n",
        "    if lang =='EN':\n",
        "        return f'\"{a}\" is to \"{b}\" as \"{c}\" is to '\n",
        "    if lang =='FR':\n",
        "        return f'\"{a}\" est à \"{b}\" ce que \"{c}\" est à '\n",
        "    if lang =='IT':\n",
        "        return f'\"{a}\" sta a \"{b}\" come \"{c}\" sta a  '\n",
        "    if lang =='ES':\n",
        "        return f'\"{a}\" es a \"{b}\" como \"{c}\" es a  '\n",
        "    if lang =='DE':\n",
        "        #Version1\n",
        "        #return f'\"{a}\" verhält sich zu \"{b}\" wie \"{c}\" zu '\n",
        "        #Version2\n",
        "        return f'\"{a}\" ist zu \"{b}\" wie \"{c}\" ist zu '\n",
        "    if lang =='PT':\n",
        "        return f'\"{a}\" está para \"{b}\" assim como \"{c}\" está para '\n",
        "    if lang == 'AL':\n",
        "        return f'\"{a}\" është për \"{b}\" ashtu si \"{c}\" për '\n",
        "    if lang == 'BM':\n",
        "        return f'\"{a}\" is to \"{b}\" as \"{c}\" is to '\n",
        "        #return f'\"{a}\" ye \"{b}\" ye i n’a fɔ \"{c}\" ye '\n",
        "    if lang == 'HR':\n",
        "        #Version 1\n",
        "        #return f'\"{a}\" je za \"{b}\" kao što je \"{c}\" za '\n",
        "        #Version 2\n",
        "        #return f'Riječ \"{a}\" je riječi \"{b}\" jednako što je riječ \"{c}\" riječi '\n",
        "        #Version 3\n",
        "        return f'Odnos između riječi \"{a}\" i \"{b}\" jednak je odnosu između riječi \"{c}\" i '\n",
        "    if lang == 'LT':\n",
        "        return f'\"{a}\" yra \"{b}\" taip, kaip \"{c}\" yra '\n",
        "    if lang == 'SL':\n",
        "        #Version 1\n",
        "        return f'Beseda \"{a}\" je besedi \"{b}\" enako, kot je beseda \"{c}\" besedi '\n",
        "        #Version 2\n",
        "        #return f'Beseda \"{a}\" je besedi \"{b}\" enako, kot je besedi ... beseda \"{c}\".'\n",
        "        #Version 3\n",
        "        #return f'\"{a}\" in \"{b}\" sta kot \"{c}\" in '\n",
        "    if lang == 'SK':\n",
        "        #Version 1\n",
        "        return f'Slovo \"{a}\"  sa má k slovu \"{b}\" ako slovo \"{c}\" k slovu '\n",
        "        #Version 2\n",
        "        #return f'Vzťah medzi slovami \"{a}\" a \"{b}\" je rovnaký ako medzi \"{c}\" a '\n",
        "        #Version 3\n",
        "        #return f'\"{a}\" sa má k \"{b}\" ako \"{c}\" k '\n",
        "    if lang == 'RO':\n",
        "        return f'\"{a}\" este pentru \"{b}\" cum \"{c}\" este pentru '\n",
        "    if lang == 'HE':\n",
        "        #Version 1\n",
        "        return f'\"{a}\" ל \"{b}\" כמו \"{c}\" ל '\n",
        "        #Version 2\n",
        "        #return f' ל \"{c}\" כ \"{b}\" ל \"{a}\"'\n",
        "    if lang == 'EL':\n",
        "        #Version 1\n",
        "        return f'το \"{a}\" είναι προς το \"{b}\" ό,τι το \"{c}\" προς το '\n",
        "        #Version2\n",
        "        #return  f'Η σχέση μεταξύ \"{a}\" και \"{b}\" είναι ίδια με τη σχέση μεταξύ \"{c}\" και '\n",
        "    if lang == 'MK':\n",
        "        #Version1\n",
        "        #return f'\"{a}\" е за \"{b}\" исто што и \"{c}\" за '\n",
        "        #Version2\n",
        "        #return f'Зборот \"{a}\" за зборот \"{b}\" е исто што и зборот \"{c}\" за зборот '\n",
        "        #Version3\n",
        "        return f'Односот меѓу зборовите \"{a}\" и \"{b}\" е еднаков со односот меѓу зборовите \"{c}\" и '"
      ],
      "metadata": {
        "id": "5PWK-9Gctv_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Analogies on a Model\n",
        "In order to test a generative model for analogy completion, the following code for querying the Huggingface Interface API can be utilised. Please be aware that the free version has severe rate limits, leading to time outs on a regular basis if too many queries are submitted."
      ],
      "metadata": {
        "id": "2Rx3heTMur3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "#Specify model you wish to prompt\n",
        "API_URL = \"https://api-inference.huggingface.co/models/bigscience/bloom\"\n",
        "headers = {\"Authorization\": \"Bearer xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"}\n",
        "\n",
        "def query(payload):\n",
        "  response = requests.post(API_URL, headers=headers, json=payload)\n",
        "  return response.json()"
      ],
      "metadata": {
        "id": "EA3lGEiP24-V"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYNqLYqwm-R1"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.shape_base import row_stack\n",
        "import random\n",
        "import pandas as pd\n",
        "from statistics import mean\n",
        "\n",
        "#Specify paths\n",
        "Path_rel_file = \"Path_to_relation_file\"\n",
        "Path_to_output_folder = \"Path_to_relation_file\"\n",
        "\n",
        "#Specify path to relation CSV with all languages, type of relation and number of analogies you wish to randomly select\n",
        "df = pd.read_csv(Path_rel_file)\n",
        "relation = \"L01_hypernyms_animals\"\n",
        "num_analogies = 30\n",
        "\n",
        "def get_analogy_response(ab_analogies, cd_analogies, lang_dict_parallel, lang_dict_all, language, num_analogies):\n",
        "  i, correct = 0, 0\n",
        "  missing_ids = 0\n",
        "  tracing, accuracies = [], []\n",
        "  while i < num_analogies:\n",
        "    ab, cd = lang_dict_parallel[ab_analogies[i]], lang_dict_parallel[cd_analogies[i]]\n",
        "    d = lang_dict_all[cd_analogies[i].split(\"__\")[0]]\n",
        "    prompt = get_template_with_quotes(language, ab[0], ab[1], cd[0])\n",
        "    tracing.append({'Language': language, 'a': ab[0], 'b': ab[1], 'c': cd[0], 'd': d['targets'], 'prompts': prompt})\n",
        "    response = query({\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\"max_new_tokens\": 10, \"do_sample\": False},\n",
        "    })\n",
        "    response = response[0]['generated_text'].replace(prompt, \"\").lower()\n",
        "    for word in d['targets']:\n",
        "      if word.lower() in response:\n",
        "        correct += 1\n",
        "        break\n",
        "    i += 1\n",
        "  accuracies.append({'Language': language, relation: str(correct/num_analogies)})\n",
        "  print(language, correct/num_analogies)\n",
        "\n",
        "  return tracing, accuracies\n",
        "\n",
        "def get_random_ids(lang_dict_parallel, num_analogies):\n",
        "  if len(lang_dict_parallel.keys()) > num_analogies*2:\n",
        "    analogies = random.sample(list(lang_dict_parallel.keys()), k=num_analogies*2)\n",
        "    ab_analogies, cd_analogies = analogies[:num_analogies], analogies[num_analogies:]\n",
        "  else:\n",
        "    ab_analogies = random.choices(list(lang_dict_parallel.keys()), k=num_analogies)\n",
        "    cd_analogies = random.choices(list(lang_dict_parallel.keys()), k=num_analogies)\n",
        "  return ab_analogies, cd_analogies\n",
        "\n",
        "def run_same_random_sample_all_languages(df, num_analogies):\n",
        "  df_parallel, df = find_parallels_across_all_languages(df.copy(), df)\n",
        "  lang_dict_parallel, lang_dict = find_all_pairs(df_parallel, df, \"EN\")\n",
        "  ab_analogies, cd_analogies = get_random_ids(lang_dict_parallel, num_analogies)\n",
        "  rows_prompts, rows_accuracies = [], []\n",
        "  print(\"Getting accuracies on analogy task for relation \"+relation+\" in:\")\n",
        "  for lang in df_parallel.columns[3:]:\n",
        "    lang_dict_parallel, lang_dict = find_all_pairs(df_parallel, df, lang)\n",
        "    rows, accuracies = get_analogy_response(ab_analogies, cd_analogies, lang_dict_parallel, lang_dict, lang, num_analogies)\n",
        "    rows_prompts.extend(rows)\n",
        "    rows_accuracies.extend(accuracies)\n",
        "\n",
        "  accuracies_df = pd.DataFrame(rows_accuracies, columns=['Language', relation])\n",
        "  accuracies_df.to_csv(Path_to_output_folder+\"accuracies_\"+relation+\".csv\")\n",
        "  prompt_df = pd.DataFrame(rows_prompts, columns=['Language', 'a', 'b', 'c', 'd', 'prompts'])\n",
        "  prompt_df.to_csv(Path_to_output_folder+\"prompts_\"+relation+\".csv\")\n",
        "\n",
        "\n",
        "run_same_random_sample_all_languages(df, num_analogies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Translation Task**"
      ],
      "metadata": {
        "id": "ClzIbgnqPF6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The translation tasks comprises using analogies in order to predict translation equivalences. This entails composing one analogy template from prompt templates in two languages, e.g. {a} is to {b} as {c} es a, where the first part represents an English template and the second a Spanish template.\n",
        "\n",
        "As a example of a filled version, the template could be \"apple is to fruit as manzana es a \" in which case the model is expected to predict \"fruta\". The following function takes a language, analogy pair a and b, and which part of the template (first or second) is needed. In the above instance, we would need to use the function as follows to compose a translation template:  \n",
        "\n",
        "```\n",
        "prompt1 = get_translation_template_with_quotes(\"EN\", apple, fruit, 1)\n",
        "prompt2 = get_translation_template_with_quotes(\"ES\", manzana, \"\", 2)\n",
        "prompt = promtp1 + prompt2\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4om3O08Fvs6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translation_template_with_quotes(lang, a, b, part):\n",
        "    if lang =='EN':\n",
        "        if part == 1:\n",
        "          return f'\"{a}\" is to \"{b}\" as '\n",
        "        if part == 2:\n",
        "          return f'\"{a}\" is to '\n",
        "    if lang =='FR':\n",
        "        if part == 1:\n",
        "          return f'\"{a}\" est à \"{b}\" ce que '\n",
        "        if part == 2:\n",
        "          return f'\"{a}\" est à '\n",
        "    if lang =='IT':\n",
        "        if part == 1:\n",
        "          return f'\"{a}\" sta a \"{b}\" come '\n",
        "        if part == 2:\n",
        "          return f'\"{a}\" sta a  '\n",
        "    if lang =='ES':\n",
        "        if part == 1:\n",
        "          return f'\"{a}\" es a \"{b}\" como '\n",
        "        if part == 2:\n",
        "          return f'\"{a}\" es a  '\n",
        "    if lang =='DE':\n",
        "        if part == 1:\n",
        "          return f'\"{a}\" ist zu \"{b}\" wie '\n",
        "        if part == 2:\n",
        "          return f'\"{a}\" ist zu '\n",
        "    if lang =='PT':\n",
        "        if part == 1:\n",
        "          return f'\"{a}\" está para \"{b}\" assim como '\n",
        "        if part == 2:\n",
        "          return f'\"{a}\" está para '\n",
        "    if lang == 'AL':\n",
        "      if part == 1:\n",
        "        return f'\"{a}\" është për \"{b}\" ashtu si '\n",
        "      if part == 2:\n",
        "        return f'\"{a}\" për '\n",
        "    if lang == 'BM':\n",
        "      if part == 1:\n",
        "        return f'\"{a}\" is to \"{b}\" as '\n",
        "      if part == 2:\n",
        "        return f'\"{a}\" is to '\n",
        "        #return f'\"{a}\" ye \"{b}\" ye i n’a fɔ \"{c}\" ye '\n",
        "    if lang == 'HR':\n",
        "      if part == 1:\n",
        "        return f'Odnos između riječi \"{a}\" i \"{b}\" jednak je '\n",
        "        #return f'Riječ \"{a}\" je riječi \"{b}\" jednako što je '\n",
        "      if part == 2:\n",
        "        return f'odnosu između riječi \"{a}\" i '\n",
        "        #return f'riječ \"{a}\" riječi '\n",
        "    if lang == 'LT':\n",
        "      if part == 1:\n",
        "        return f'\"{a}\" yra \"{b}\" taip, '\n",
        "      if part == 2:\n",
        "        return f'\"kaip \"{a}\" yra '\n",
        "    if lang == 'SL':\n",
        "      if part == 1:\n",
        "        return f'Beseda \"{a}\" je besedi \"{b}\" enako, kot je '\n",
        "      if part == 2:\n",
        "        return f'beseda \"{a}\" besedi '\n",
        "    if lang == 'SK':\n",
        "      if part == 1:\n",
        "        return f'Slovo \"{a}\"  sa má k slovu \"{b}\" ako '\n",
        "      if part == 2:\n",
        "        return f'slovo \"{a}\" k slovu '\n",
        "        #Version 2\n",
        "    if lang == 'RO':\n",
        "      if part == 1:\n",
        "        return f'\"{a}\" este pentru \"{b}\" cum '\n",
        "      if part == 2:\n",
        "        return f'\"{a}\" este pentru '\n",
        "    if lang == 'HE':\n",
        "        #Version 1\n",
        "      if part == 1:\n",
        "        return f'\"{a}\" ל \"{b}\" כ '\n",
        "      if part == 2:\n",
        "        return f' מו \"{a}\" ל '\n",
        "        #Version 2\n",
        "        #return f' ל \"{c}\" כ \"{b}\" ל \"{a}\"'\n",
        "    if lang == 'EL':\n",
        "        #Version 1\n",
        "      if part == 1:\n",
        "        return f'το \"{a}\" είναι προς το \"{b}\" ό,τι '\n",
        "      if part == 2:\n",
        "        return f'το \"{a}\" προς το '\n",
        "    if lang == 'MK':\n",
        "      if part == 1:\n",
        "        return f'Односот меѓу зборовите \"{a}\" и \"{b}\" е '\n",
        "      if part == 2:\n",
        "        return f'еднаков со односот меѓу зборовите \"{c}\" и '"
      ],
      "metadata": {
        "id": "wBuog8_-Q-Td"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining parallel dictionary\n",
        "\n",
        "In order to compose analogy-based translation prompts, we require all parallel pairs in the two languages that do not contain \"DUPLICATE\" or \"NO_TRANSLATION\" strings (see the paper for more details).\n",
        "\n",
        "The following function returns two ID- and English-aligned dictionaries of pairs in the requested languages. Since the data structure is slightly different for English, the function needs to consider this if one of the languages requested is English."
      ],
      "metadata": {
        "id": "dZeTj-tMxppw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.groupby.generic import DataFrameGroupBy\n",
        "import pandas as pd\n",
        "\n",
        "def get_pairs(df, lang1, lang2):\n",
        "  lang1_dict, lang2_dict = {}, {}\n",
        "  for i, row in df.iterrows():\n",
        "    if type(row['ID']) == str:\n",
        "      ID = row['ID']\n",
        "      en_source_word = row['EN'].replace(\"_\", \" \")\n",
        "      lang1_source_word = row[lang1].replace(\"_\", \" \")\n",
        "      lang2_source_word = row[lang2].replace(\"_\", \" \")\n",
        "    else:\n",
        "      if type(row['EN']) == str:\n",
        "        lang1_dict[ID+\"__\"+en_source_word+\"_\"+row['EN']] = [lang1_source_word, row[lang1].replace(\"_\", \" \")]\n",
        "        lang2_dict[ID+\"__\"+en_source_word+\"_\"+row['EN']] = [lang2_source_word, row[lang2].replace(\"_\", \" \")]\n",
        "\n",
        "  return lang1_dict, lang2_dict\n",
        "\n",
        "\n",
        "def reshape_en_data_stucture(df):\n",
        "  #Reshape data structure to make EN fit the format of the other languages and rename column Target to EN\n",
        "  df.loc[df['ID'].notna(), 'Target'] = df['Source']\n",
        "  df.rename(columns={'Target': 'EN'}, inplace = True)\n",
        "  return df\n",
        "\n",
        "def find_translation_pairs(df, lang1, lang2):\n",
        "  lang1_dict, lang2_dict = {}, {}\n",
        "\n",
        "  #Remove all lines with empty values, \"DUPLICATE\" or \"NO_TRANSLATION\" in either language\n",
        "  subset = df[df[lang1].notna() & df[lang2].notna()]\n",
        "  subset = subset[['ID','Relation','Source', 'EN', lang1, lang2]] if lang1 != \"EN\" and lang2 != \"EN\" else subset[['ID','Relation','Source', lang1, lang2]]\n",
        "  subset = subset[~subset[lang1].str.contains(\"DUPLICATE|NO_TRANSLATION\") & ~subset[lang2].str.contains(\"DUPLICATE|NO_TRANSLATION\")]\n",
        "\n",
        "  #Use the following function to display the dataframe\n",
        "  #display(df)\n",
        "  lang1_dict, lang2_dict = get_pairs(subset, lang1, lang2)\n",
        "\n",
        "  return lang1_dict, lang2_dict"
      ],
      "metadata": {
        "id": "1DXjnuO3hKO2"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt a model with analogy-based templates\n",
        "\n",
        "Specify the type of language pairs you wish to test on the analogy-based translation task, specify the languages, and the type of model you wish to use.\n"
      ],
      "metadata": {
        "id": "RKpBu6go3sq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.shape_base import row_stack\n",
        "import random\n",
        "import pandas as pd\n",
        "import requests\n",
        "from statistics import mean\n",
        "\n",
        "#Specify path to file, type of relation (for output file), language1 and language2 to be used in the analogy-based translation task\n",
        "df = pd.read_csv(Path_rel_file)\n",
        "df = reshape_en_data_stucture(df)\n",
        "\n",
        "def find_translation(lang_dict1, lang_dict2, language1, language2):\n",
        "  correct, corr = 0, 0\n",
        "  tracing, accuracies = [], []\n",
        "  print(\"Translating from \", language1, \"to \", language2)\n",
        "  for id in lang_dict1.keys():\n",
        "    a, b = lang_dict1[id]\n",
        "    c, d = lang_dict2[id]\n",
        "    prompt_part1 = get_translation_template_with_quotes(language1, a, b, 1)\n",
        "    prompt_part2 = get_translation_template_with_quotes(language2, c, \"\", 2)\n",
        "    prompt = prompt_part1+prompt_part2\n",
        "    response = query({\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\"max_new_tokens\": 10, \"do_sample\": False},\n",
        "    })\n",
        "    response = response[0]['generated_text'].replace(prompt, \"\").lower()\n",
        "    if d.lower() in response:\n",
        "      correct += 1\n",
        "      corr = 1\n",
        "    tracing.append({'Host': language1, 'Transfer': language2, 'a': a, 'b': b, 'c': c, 'd': d, 'prompts': prompt, 'response': response, 'corect': corr})\n",
        "  print(\"Number of templates: \",len(lang_dict1.keys()), \"Accuracy: \", correct/len(lang_dict1.keys()))\n",
        "  accuracies.append({'Transfer': language1+\"=>\"+language2, 'Accuracy':correct/len(lang_dict1.keys())})\n",
        "  corr = 0\n",
        "  prompt_df = pd.DataFrame(tracing, columns=['Source', 'Transfer', 'a', 'b', 'c', 'd', 'prompts', 'response'])\n",
        "  prompt_df.to_csv(Path_to_output_folder+\"/translation_experiment_\"+language1+\"_\"+language2+\"_\"+relation+\".csv\")\n",
        "  return accuracies\n",
        "\n",
        "#List of language combinations tested in the publication\n",
        "rows_accuracies = []\n",
        "for pair in [[\"EN\",\"ES\"],[\"EN\", \"DE\"],[\"EN\",\"FR\"],[\"FR\",\"EN\"],[\"FR\",\"RO\"],[\"ES\",\"PT\"],[\"ES\",\"IT\"],[\"HR\",\"SL\"],[\"HR\",\"SK\"],[\"SK\",\"HR\"], [\"EN\",\"BM\"], [\"EN\",\"AL\"],[\"EN\",\"EL\"],[\"EN\",\"HE\"],[\"FR\",\"BM\"]]:\n",
        "  language1, language2 = pair[0], pair[1]\n",
        "  lang1_dict, lang2_dict = find_translation_pairs(df, language1, language2)\n",
        "  rows_accuracies.extend(find_translation(lang1_dict, lang2_dict, language1, language2))\n",
        "\n",
        "  #Output all accuracies for this relation to CSV\n",
        "  accuracies_df = pd.DataFrame(rows_accuracies, columns=['Language', relation])\n",
        "  accuracies_df.to_csv(Path_to_output_folder+\"translation_acc_\"+relation+\".csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6NT48opt3rrV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HdYUa89IJcTxH3StrigjPO1XBp99NhGm",
      "authorship_tag": "ABX9TyMG+3BPHwMU1mO7SkyFiWsX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}